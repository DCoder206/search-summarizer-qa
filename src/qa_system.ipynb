{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mlenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-19 02:17:34.005270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739911654.025575   21980 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739911654.031519   21980 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-19 02:17:34.053137: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline,BartTokenizer,AutoTokenizer,AutoModelForQuestionAnswering,AutoModelForSeq2SeqLM,AutoModelForSequenceClassification,AutoModel\n",
    "import requests,re,nltk,torch,numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from os.path import exists\n",
    "from random import sample as random_sample\n",
    "from spacy import load as spc_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionManager:\n",
    "    def __init__(self, max_tokens=1024):\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "        self.max_tokens = max_tokens\n",
    "        self.context = \"\"\n",
    "\n",
    "    def update_context(self, new_content):\n",
    "        combined = f\"{self.context}\\n{new_content}\".strip()\n",
    "        tokens = self.tokenizer.tokenize(combined)\n",
    "        if len(tokens) > self.max_tokens:\n",
    "            keep_tokens = int(self.max_tokens * 0.35)\n",
    "            new_tokens = self.tokenizer.tokenize(new_content)\n",
    "            tokens = tokens[:keep_tokens] + new_tokens[-(self.max_tokens - keep_tokens):]\n",
    "            \n",
    "        self.context = self.tokenizer.convert_tokens_to_string(tokens)\n",
    "    \n",
    "    def get_context(self): return self.context.strip()\n",
    "session = SessionManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who discovered the electron\n"
     ]
    }
   ],
   "source": [
    "def extract_text(html):\n",
    "    text = re.sub(r'<[^>]*>',' ',html)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text = re.sub(r'[?:;\\'\"(){}[\\]<>|@&\\/\\\\]','',text)\n",
    "    text = re.sub(r'%[^%]*%','',text)\n",
    "    text = re.sub(r'%','',text)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if not (len(word) > 45 or (word.isupper() and len(word) > 32))]\n",
    "    text = \" \".join(filtered_words)\n",
    "    return text.strip()\n",
    "session = SessionManager()\n",
    "user_query = input(\"Enter your query:\").lower()\n",
    "print(user_query)\n",
    "full_query = f\"{session.get_context()}\\n\\nCurrent Query: {user_query}\"\n",
    "search_url = f\"https://html.duckduckgo.com/html/?q={'+'.join(user_query.split(' '))}\"\n",
    "response = requests.get(search_url,headers={\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:135.0) Gecko/20100101 Firefox/135.0\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_urls(response):\n",
    "    soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "    sresults = []\n",
    "    for i,result in enumerate(soup.find_all(\"div\",class_=\"result\"),start=1):\n",
    "        if i > 6: break\n",
    "        title = result.find(\"a\",class_=\"result__a\")\n",
    "        if not title:continue\n",
    "        tag = result.find(\"a\",class_=\"result__snippet\")\n",
    "        snip = tag.text.strip() if tag else None\n",
    "        sresults.append({\n",
    "            \"id\":i,\n",
    "            \"url\":title[\"href\"],\n",
    "            \"description\":snip\n",
    "        })\n",
    "    return sresults\n",
    "search_results = res_urls(response)\n",
    "content_dict = {}\n",
    "for result in search_results:\n",
    "    try:\n",
    "        url = result[\"url\"]\n",
    "        mch = re.search(r\"uddg=([^&]+)\",url)\n",
    "        if mch:url = urllib.parse.unquote(mch.group(1))\n",
    "        page_response = requests.get(url,headers={\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:135.0) Gecko/20100101 Firefox/135.0\"})\n",
    "        soup = BeautifulSoup(page_response.text,\"html.parser\")\n",
    "        for element in soup([\"script\",\"style\",\"nav\",\"footer\",\"header\"]):element.decompose()\n",
    "        page_text = extract_text(page_response.text)[:7500]\n",
    "        if page_text:\n",
    "            content_dict[result['id']] = page_text\n",
    "    except Exception:continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\",model=\"cross-encoder/nli-roberta-base\")\n",
    "def filter_relevant_sentences(text, query, threshold=0.7):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    candidate_labels = [\"entailment\", \"contradiction\"]\n",
    "    hypothesis_template = f\"This sentence {{}} is relevant to the query: {query}\"\n",
    "    valid_sentences = [s for s in sentences if s.strip()]\n",
    "    if not valid_sentences:\n",
    "        return \"\"\n",
    "    \n",
    "    results = classifier(\n",
    "        valid_sentences, \n",
    "        candidate_labels,\n",
    "        hypothesis_template=hypothesis_template,\n",
    "        batch_size=4\n",
    "    )\n",
    "    \n",
    "    filtered_sentences = []\n",
    "    for sentence, result in zip(valid_sentences, results):\n",
    "        score = result[\"scores\"][result[\"labels\"].index(\"entailment\")]\n",
    "        if score >= threshold:\n",
    "            filtered_sentences.append(sentence)\n",
    "    \n",
    "    return \" \".join(filtered_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer_sum = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "summarizer = pipeline(\"summarization\",model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text,max_tokens=512):\n",
    "    global tokenizer_sum\n",
    "    sentences = nltk.sent_tokenize(text,language=\"english\")\n",
    "    full_text_token_count = len(tokenizer_sum.tokenize(text))\n",
    "    if full_text_token_count <= max_tokens: return [text]\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer_sum.tokenize(sentence)\n",
    "        sentence_token_count = len(sentence_tokens)\n",
    "        if current_length + sentence_token_count <= max_tokens:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_token_count\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_length = sentence_token_count\n",
    "            else:\n",
    "                chunks.append(sentence[:512])\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "    if current_chunk:chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "def summarize_large_text(text,query,max_chunk_tokens=512,threshold=0.7):\n",
    "    filtered_text = filter_relevant_sentences(text,query,threshold)\n",
    "    if not filtered_text.strip():\n",
    "        return \"\"\n",
    "    chunks = chunk_text(filtered_text,max_chunk_tokens)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        if not chunk.strip():continue\n",
    "        try:\n",
    "            input_tokens = tokenizer_sum.tokenize(chunk)\n",
    "            input_length = len(input_tokens)\n",
    "            adjusted_max_length = min(200,input_length) if input_length > 10 else 10\n",
    "            adjusted_min_length = min(50,adjusted_max_length - 1) if adjusted_max_length > 1 else 0\n",
    "\n",
    "            summary = summarizer(\n",
    "                chunk,\n",
    "                max_length=adjusted_max_length,\n",
    "                min_length=adjusted_min_length,\n",
    "                do_sample=False,\n",
    "                truncation=True\n",
    "            )[0][\"summary_text\"]\n",
    "            summaries.append(summary)\n",
    "        except Exception:continue\n",
    "    if not summaries: return \"\"\n",
    "    final_summary = \" \".join(summaries)\n",
    "    while len(tokenizer_sum.tokenize(final_summary)) > 512:\n",
    "        input_tokens = tokenizer_sum.tokenize(final_summary)\n",
    "        input_length = len(input_tokens)\n",
    "        adjusted_max_length = min(512,input_length)\n",
    "        adjusted_min_length = min(100,adjusted_max_length - 1) if adjusted_max_length > 1 else 0\n",
    "        final_summary = summarizer(\n",
    "            final_summary,\n",
    "            max_length=adjusted_max_length,\n",
    "            min_length=adjusted_min_length,\n",
    "            do_sample=False,\n",
    "            truncation=True\n",
    "        )[0][\"summary_text\"]\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "page_summaries = {}\n",
    "for doc_id,text in content_dict.items():\n",
    "    page_prompt = (\n",
    "        f\"Extract information relevant to '{user_query}' from this text:\\n\\n{text}\"\n",
    "    )\n",
    "    page_summary = summarize_large_text(page_prompt,user_query)\n",
    "    page_summaries[doc_id] = page_summary\n",
    "combined_summaries = \" \".join(page_summaries.values())\n",
    "final_prompt = (\n",
    "    f\"Considering previous context: {session.get_context()}\\n\"\n",
    "    f\"Synthesize information relevant to '{user_query}' from these summaries:\\n\\n{combined_summaries}\"\n",
    ")\n",
    "sumry = summarize_large_text(final_prompt,full_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_emb = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model_emb = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").eval()\n",
    "tokenizer_rk = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "model_rk = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "def get_embedding(text):\n",
    "    global tokenizer_emb,model_emb\n",
    "    inputs = tokenizer_emb(text,return_tensors=\"pt\",truncation=True,max_length=256)\n",
    "    with torch.no_grad():outputs = model_emb(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    masked_embeddings = embeddings * attention_mask.unsqueeze(-1).float()\n",
    "    mean_embedding = masked_embeddings.sum(dim=1) / attention_mask.sum(dim=1,keepdim=True).clamp(min=1e-9)\n",
    "    return mean_embedding[0].numpy()\n",
    "def rerank(query,contexts,top_k=4):\n",
    "    global tokenizer_rk,model_rk\n",
    "    scores = []\n",
    "    for context in contexts:\n",
    "        inputs = tokenizer_rk(query,context,return_tensors=\"pt\",truncation=True)\n",
    "        with torch.no_grad():\n",
    "            scores.append(model_rk(**inputs).logits.item())\n",
    "    return [c for _,c in sorted(zip(scores,contexts),reverse=True)[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chunks = chunk_text(\" \".join(content_dict.values()),256)\n",
    "chunk_embeds = np.array([get_embedding(chunk) for chunk in rag_chunks])\n",
    "query_embed = get_embedding(user_query).reshape(1,-1)\n",
    "similarity_scores = cosine_similarity(query_embed,chunk_embeds)[0]\n",
    "top_k = 4\n",
    "ini_indices = np.argsort(similarity_scores)[:top_k]\n",
    "ini_context = [rag_chunks[i] for i in ini_indices]\n",
    "retrieved_context = \" \".join(rerank(user_query,ini_context,top_k=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.J. Thomson\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\",model=\"distilbert-base-cased-distilled-squad\")\n",
    "query_context = f\"{sumry}\\n\\n{retrieved_context}\"\n",
    "qa_result = qa_pipeline(question=user_query,context=query_context)\n",
    "print(qa_result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.update_context(\n",
    "    f\"query: {user_query}\\nanswer: {qa_result['answer']}\\n\"\n",
    "    f\"summary context: {sumry}\\nretrieved Context: {retrieved_context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCETrainer:\n",
    "    def __init__(self, model, tokenizer, learning_rate=4e-5):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def update(self,inputs,responses,rewards):\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(self.model.device)\n",
    "        \n",
    "        if isinstance(self.model, AutoModelForQuestionAnswering):\n",
    "            outputs = self.model(**inputs)\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "            \n",
    "            start_probs = torch.nn.functional.log_softmax(start_logits, dim=-1)\n",
    "            end_probs = torch.nn.functional.log_softmax(end_logits, dim=-1)\n",
    "            \n",
    "            start_positions = responses[\"start_positions\"]\n",
    "            end_positions = responses[\"end_positions\"]\n",
    "            \n",
    "            start_log_probs = start_probs.gather(-1, start_positions.unsqueeze(-1)).squeeze(-1)\n",
    "            end_log_probs = end_probs.gather(-1, end_positions.unsqueeze(-1)).squeeze(-1)\n",
    "            total_log_probs = (start_log_probs + end_log_probs) / 2\n",
    "\n",
    "            loss = -torch.mean(total_log_probs * rewards_tensor)\n",
    "\n",
    "        elif isinstance(self.model, AutoModelForSequenceClassification):\n",
    "            outputs = self.model(**inputs)\n",
    "            log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)\n",
    "            response_labels = responses[\"labels\"]\n",
    "            selected_log_probs = log_probs.gather(-1, response_labels.unsqueeze(-1)).squeeze(-1)\n",
    "            loss = -torch.mean(selected_log_probs * rewards_tensor)\n",
    "\n",
    "        else:\n",
    "            outputs = self.model(**inputs, labels=responses[\"input_ids\"])\n",
    "            log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)\n",
    "            response_ids = responses[\"input_ids\"]\n",
    "            response_mask = (response_ids != self.tokenizer.pad_token_id).float()\n",
    "            nll = torch.nn.functional.nll_loss(\n",
    "                log_probs.view(-1, log_probs.size(-1)),\n",
    "                response_ids.view(-1),\n",
    "                reduction='none'\n",
    "            )\n",
    "            log_probs = -nll.view(response_ids.size()) * response_mask\n",
    "            avg_log_probs = (log_probs.sum(dim=-1) / response_mask.sum(dim=-1)).mean()\n",
    "            loss = -avg_log_probs * rewards_tensor.mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_factors = {\n",
    "    \"qa\":0.5,\n",
    "    \"relevance\":0.3,\n",
    "    \"summarizer\":0.2\n",
    "}\n",
    "\n",
    "model_paths = {\n",
    "    \"summarizer\":\"../models/fine_tuned_bart\",\n",
    "    \"qa\":\"../models/fine_tuned_qa\",\n",
    "    \"relevance\":\"../models/fine_tuned_relevance\"\n",
    "}\n",
    "\n",
    "def init_model(model_name,model_class):\n",
    "    if exists(model_paths[model_name]):\n",
    "        model = model_class.from_pretrained(model_paths[model_name])\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_paths[model_name])\n",
    "    else:\n",
    "        model = model_class.from_pretrained({\n",
    "            \"summarizer\":\"facebook/bart-base\",\n",
    "            \"qa\":\"distilbert-base-cased-distilled-squad\",\n",
    "            \"relevance\":\"cross-encoder/nli-roberta-base\"\n",
    "        }[model_name])\n",
    "        tokenizer = AutoTokenizer.from_pretrained({\n",
    "            \"summarizer\":\"facebook/bart-base\",\n",
    "            \"qa\":\"distilbert-base-cased-distilled-squad\",\n",
    "            \"relevance\":\"cross-encoder/nli-roberta-base\"\n",
    "        }[model_name])\n",
    "    return model,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summarizer,tokenizer_summarizer = init_model(\"summarizer\",AutoModelForSeq2SeqLM)\n",
    "model_qa,tokenizer_qa = init_model(\"qa\",AutoModelForQuestionAnswering)\n",
    "model_relevance,tokenizer_relevance = init_model(\"relevance\",AutoModelForSequenceClassification)\n",
    "relevance_inputs = tokenizer_relevance(text=[user_query],text_pair=[retrieved_context],padding=True,truncation=True,return_tensors=\"pt\")\n",
    "trainers = {\n",
    "    \"summarizer\": REINFORCETrainer(model_summarizer, tokenizer_summarizer),\n",
    "    \"qa\": REINFORCETrainer(model_qa, tokenizer_qa),\n",
    "    \"relevance\": REINFORCETrainer(model_relevance, tokenizer_relevance)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback():\n",
    "    try:\n",
    "        feedback = int(input(\"Rate the answer quality (1-5):\"))\n",
    "        return max(1,min(5,feedback))\n",
    "    except ValueError:\n",
    "        return 3\n",
    "user_rating = get_feedback()\n",
    "user_rating = max(1,min(5,user_rating))\n",
    "base_reward = (user_rating - 3) / 2\n",
    "rewards = {k:base_reward * v for k,v in rl_factors.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_indices = np.argsort(similarity_scores)[:top_k]\n",
    "positive_chunks = [rag_chunks[i] for i in ini_indices]\n",
    "negative_chunks = [rag_chunks[i] for i in negative_indices]\n",
    "context_pairs = [(user_query, chunk, 1) for chunk in positive_chunks] + \\\n",
    "                [(user_query, chunk, 0) for chunk in negative_chunks]\n",
    "np.random.shuffle(context_pairs)\n",
    "\n",
    "relevance_inputs = tokenizer_relevance.batch_encode_plus(\n",
    "    [(session.get_context(), c) for _, c, _ in context_pairs],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "relevance_labels = torch.tensor([l for _, _, l in context_pairs]).long()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model_relevance(**relevance_inputs).logits\n",
    "    preds = torch.sigmoid(logits[:, 0]).round().squeeze()\n",
    "    labels = torch.tensor([l for _, _, l in context_pairs]).float()\n",
    "    accuracy = (preds == labels).float().mean()\n",
    "rewards[\"relevance\"] = accuracy.item() * rewards[\"relevance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qa_chunks = chunk_text(\" \".join(content_dict.values()), 256)\n",
    "query_context_chunks = chunk_text(query_context, 256)\n",
    "\n",
    "negative_qa_chunks = [c for c in all_qa_chunks if c not in query_context_chunks]\n",
    "adversarial_qa_chunks = random_sample(negative_qa_chunks, min(2, len(negative_qa_chunks)))\n",
    "\n",
    "augmented_qa_context = f\"{query_context}[ADV]{'[ADV]'.join(adversarial_qa_chunks)}\"\n",
    "\n",
    "answer_embed = get_embedding(qa_result[\"answer\"])\n",
    "context_embed = get_embedding(augmented_qa_context)\n",
    "consistency_score = cosine_similarity([answer_embed], [context_embed])[0][0]\n",
    "qa_reward = (\n",
    "    0.5 * base_reward +\n",
    "    0.3 * consistency_score +\n",
    "    0.2 * cosine_similarity(\n",
    "        [get_embedding(qa_result[\"answer\"])],\n",
    "        [get_embedding(session.get_context())]\n",
    "    )[0][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spc_load(\"en_core_web_md\")\n",
    "doc_source = nlp(combined_summaries)\n",
    "doc_summary = nlp(sumry)\n",
    "source_ents = set((ent.text.lower(), ent.label_) for ent in doc_source.ents)\n",
    "summary_ents = set((ent.text.lower(), ent.label_) for ent in doc_summary.ents)\n",
    "entity_retention = len(summary_ents & source_ents) / len(source_ents) if source_ents else 1.0\n",
    "\n",
    "compression_ratio = len(sumry) / len(combined_summaries)\n",
    "compression_score = max(0, 1 - abs(0.45 - compression_ratio))\n",
    "\n",
    "summarizer_reward = (\n",
    "    0.3 * base_reward +\n",
    "    0.4 * entity_retention +\n",
    "    0.2 * compression_score +\n",
    "    0.1 * cosine_similarity(\n",
    "        [get_embedding(sumry)],\n",
    "        [get_embedding(session.get_context())]\n",
    "    )[0][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m trainers[input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqa\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m     45\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minput_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     46\u001b[0m         responses\u001b[38;5;241m=\u001b[39minput_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     47\u001b[0m         rewards\u001b[38;5;241m=\u001b[39minput_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     48\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m, in \u001b[0;36mREINFORCETrainer.update\u001b[0;34m(self, inputs, responses, rewards)\u001b[0m\n\u001b[1;32m     32\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(selected_log_probs \u001b[38;5;241m*\u001b[39m rewards_tensor)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, labels\u001b[38;5;241m=\u001b[39m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     36\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m     response_ids \u001b[38;5;241m=\u001b[39m responses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_ids'"
     ]
    }
   ],
   "source": [
    "inputs = (\n",
    "    {\n",
    "        \"name\": \"summarizer\",\n",
    "        \"query\": tokenizer_summarizer(final_prompt, return_tensors=\"pt\", truncation=True),\n",
    "        \"response\": tokenizer_summarizer(sumry, return_tensors=\"pt\"),\n",
    "        \"reward\": [summarizer_reward * rl_factors[\"summarizer\"]]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"qa\",\n",
    "        \"query\": tokenizer_qa(\n",
    "            user_query,\n",
    "            augmented_qa_context,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ),\n",
    "        \"response\": {\n",
    "            \"start_positions\": torch.tensor([qa_result[\"start\"]]),\n",
    "            \"end_positions\": torch.tensor([qa_result[\"end\"]])\n",
    "        },\n",
    "        \"reward\": [qa_reward * rl_factors[\"qa\"]]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"relevance\",\n",
    "        \"query\": tokenizer_relevance.batch_encode_plus(\n",
    "            [(session.get_context(), c) for _, c, _ in context_pairs],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ),\n",
    "        \"response\": relevance_labels,\n",
    "        \"reward\": [rewards[\"relevance\"] * rl_factors[\"relevance\"]]\n",
    "    }\n",
    ")\n",
    "for input_dict in inputs:\n",
    "    trainer = trainers[input_dict[\"name\"]]\n",
    "    if input_dict[\"name\"] == \"qa\":\n",
    "        loss = trainer.update(\n",
    "            inputs=input_dict[\"query\"],\n",
    "            responses=input_dict[\"response\"],\n",
    "            rewards=input_dict[\"reward\"]\n",
    "        )\n",
    "    else:\n",
    "        loss = trainer.update(\n",
    "            inputs=input_dict[\"query\"],\n",
    "            responses=input_dict[\"response\"],\n",
    "            rewards=input_dict[\"reward\"]\n",
    "        )\n",
    "\n",
    "for model_name in model_paths:\n",
    "    trainers[model_name].model.save_pretrained(model_paths[model_name])\n",
    "    trainers[model_name].tokenizer.save_pretrained(model_paths[model_name])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
